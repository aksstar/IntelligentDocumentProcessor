{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a7e2cc-6bc1-4e5f-bc84-9d28d456b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.37.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.116.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.10.4 (from gradio)\n",
      "  Downloading gradio_client-1.10.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
      "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.3.1)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.11.7)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.12.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.46.2)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.10.4->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.10.4->gradio) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.28.1->gradio)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (1.26.20)\n",
      "Downloading gradio-5.37.0-py3-none-any.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.4-py3-none-any.whl (323 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.12.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, tomlkit, shellingham, semantic-version, ruff, orjson, hf-xet, groovy, ffmpy, huggingface-hub, typer, safehttpx, gradio-client, gradio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [gradio]13/14\u001b[0m [gradio]face-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ffmpy-0.6.0 gradio-5.37.0 gradio-client-1.10.4 groovy-0.1.2 hf-xet-1.1.5 huggingface-hub-0.33.4 orjson-3.10.18 pydub-0.25.1 ruff-0.12.3 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.13.3 typer-0.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051ff24-d1d7-42bd-bee1-26b1cab9041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3a37d42fbd334aaee8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3a37d42fbd334aaee8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Readability - Image bytes length: 61432, MIME type: image/jpeg\n",
      "DEBUG: Rotation - Image bytes length: 61432, MIME type: image/jpeg\n",
      "DEBUG: Original image rotated by 270 degrees CLOCKWISE.\n",
      "DEBUG: Bounding Box Detection - Image bytes length: 61311, MIME type: image/jpeg\n",
      "DEBUG: Rotated image cropped to largest bounding box for extraction/validation: (682, 158, 933, 367)\n",
      "DEBUG: Enhancement - Image bytes length: 10895, MIME type: image/jpeg\n",
      "DEBUG: Extraction - Image bytes length: 10895, MIME type: image/jpeg\n",
      "DEBUG: Readability - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Rotation - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Rotation analysis did not recommend rotation or angle was invalid/not found.\n",
      "DEBUG: Bounding Box Detection - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Rotated image cropped to largest bounding box for extraction/validation: (913, 419, 1183, 699)\n",
      "DEBUG: Enhancement - Image bytes length: 8421, MIME type: image/jpeg\n",
      "DEBUG: Extraction - Image bytes length: 8421, MIME type: image/jpeg\n",
      "DEBUG: Readability - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Rotation - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Rotation analysis did not recommend rotation or angle was invalid/not found.\n",
      "DEBUG: Cropping disabled by user. Using full rotated image for extraction/validation/enhancement.\n",
      "DEBUG: Enhancement - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Extraction - Image bytes length: 96368, MIME type: image/jpeg\n",
      "DEBUG: Readability - Image bytes length: 61432, MIME type: image/jpeg\n",
      "DEBUG: Rotation - Image bytes length: 61432, MIME type: image/jpeg\n",
      "DEBUG: Original image rotated by 270 degrees CLOCKWISE.\n",
      "DEBUG: Cropping disabled by user. Using full rotated image for extraction/validation/enhancement.\n",
      "DEBUG: Enhancement - Image bytes length: 61311, MIME type: image/jpeg\n",
      "DEBUG: Extraction - Image bytes length: 61311, MIME type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import io\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import traceback # Added for detailed error reporting\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel # Import BaseModel for BoundingBox\n",
    "\n",
    "# Helper class to represent a bounding box (from user's reference)\n",
    "class BoundingBox(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a bounding box with its 2D coordinates and associated label.\n",
    "    Attributes:\n",
    "        box_2d (list[int]): A list of integers representing the 2D coordinates of the bounding box,\n",
    "                            typically in the format [y_min, x_min, y_max, x_max].\n",
    "        label (str): A string representing the label or class associated with the object within the bounding box.\n",
    "    \"\"\"\n",
    "    box_2d: list[int]\n",
    "    label: str\n",
    "\n",
    "\n",
    "# --- Modified Functions for Gradio Compatibility ---\n",
    "\n",
    "def _infer_mime_type_and_convert_to_bytes(image_pil: Image.Image):\n",
    "    \"\"\"\n",
    "    Infers the MIME type and converts a PIL Image object to bytes.\n",
    "    Returns (bytes, mime_type).\n",
    "    \"\"\"\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    mime_type = \"image/jpeg\" # Default\n",
    "    format = \"JPEG\"\n",
    "\n",
    "    # Check if image has an alpha channel (RGBA) or is a PNG-like format\n",
    "    if image_pil.mode in ('RGBA', 'LA') or (image_pil.mode == 'P' and 'transparency' in image_pil.info):\n",
    "        mime_type = \"image/png\"\n",
    "        format = \"PNG\"\n",
    "    \n",
    "    try:\n",
    "        # Save the image to the BytesIO object in the determined format\n",
    "        image_pil.save(img_byte_arr, format=format)\n",
    "        img_byte_arr.seek(0) # Rewind to the beginning\n",
    "        return img_byte_arr.getvalue(), mime_type\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PIL image to bytes: {e}\")\n",
    "        return None, None # Return None if conversion fails\n",
    "\n",
    "\n",
    "def analyze_image_text_readability_gradio(image_pil: Image.Image, project_id: str, location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if text in the image (provided as PIL Image) is readable using Gemini API.\n",
    "    \"\"\"\n",
    "    if image_pil is None:\n",
    "        return {\"error\": \"No image provided for readability check.\"}\n",
    "\n",
    "    img_bytes, mime_type = _infer_mime_type_and_convert_to_bytes(image_pil)\n",
    "    if img_bytes is None:\n",
    "        return {\"error\": \"Failed to convert image for API call.\"}\n",
    "    \n",
    "    print(f\"DEBUG: Readability - Image bytes length: {len(img_bytes)}, MIME type: {mime_type}\") # Debug print\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "        \n",
    "        # Changed from types.Part.from_data to types.Part.from_bytes as requested\n",
    "        try:\n",
    "            image_part = types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n",
    "        except Exception as e_part:\n",
    "            print(f\"ERROR: types.Part.from_bytes failed: {e_part}\")\n",
    "            traceback.print_exc() # Print full traceback here for detailed error\n",
    "            return {\"error\": f\"Failed to create image part for API: {e_part}\"}\n",
    "\n",
    "\n",
    "        prompt_text = (\n",
    "            \"Analyze the text within this image. Provide a JSON object with two fields: \"\n",
    "            \"'is_readable' (boolean, true if the text is clearly legible and easily understandable, false otherwise) \"\n",
    "            \"and 'reason' (string, explaining why the text is or isn't readable, including specific details like \"\n",
    "            \"blurriness, font size, lighting, or obstruction if applicable). Focus solely on the text's readability.\"\n",
    "        )\n",
    "\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[{\"text\": prompt_text}, image_part])\n",
    "        ]\n",
    "\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=1,\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"is_readable\": {\"type\": \"BOOLEAN\"},\n",
    "                    \"reason\": {\"type\": \"STRING\"}\n",
    "                },\n",
    "                \"required\": [\"is_readable\", \"reason\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                parsed_json = json.loads(full_response_text)\n",
    "                if \"is_readable\" in parsed_json and \"reason\" in parsed_json:\n",
    "                    return parsed_json\n",
    "                else:\n",
    "                    return {\n",
    "                        \"error\": \"Unexpected JSON structure from API. Missing 'is_readable' or 'reason'.\",\n",
    "                        \"api_response\": parsed_json\n",
    "                    }\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": f\"Failed to parse JSON response from API: {e}\",\n",
    "                    \"raw_response_text\": full_response_text\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"No response text received from Gemini API.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Capture the full exception for better debugging\n",
    "        print(f\"ERROR: An error occurred during API call or client setup: {e}\")\n",
    "        traceback.print_exc() # Print full traceback here for detailed error\n",
    "        return {\"error\": f\"An error occurred during API call or client setup: {e}\"}\n",
    "\n",
    "\n",
    "def analyze_image_rotation_gradio(image_pil: Image.Image, project_id: str, location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Determines if an image needs rotation and suggests an angle (0, 90, 180, or 270 degrees) CLOCKWISE.\n",
    "    \"\"\"\n",
    "    if image_pil is None:\n",
    "        return {\"error\": \"No image provided for rotation analysis.\"}\n",
    "\n",
    "    img_bytes, mime_type = _infer_mime_type_and_convert_to_bytes(image_pil)\n",
    "    if img_bytes is None:\n",
    "        return {\"error\": \"Failed to convert image for API call (rotation analysis).\"}\n",
    "\n",
    "    print(f\"DEBUG: Rotation - Image bytes length: {len(img_bytes)}, MIME type: {mime_type}\")\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "        try:\n",
    "            image_part = types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n",
    "        except Exception as e_part:\n",
    "            print(f\"ERROR: types.Part.from_bytes failed for rotation analysis: {e_part}\")\n",
    "            traceback.print_exc()\n",
    "            return {\"error\": f\"Failed to create image part for rotation API: {e_part}\"}\n",
    "\n",
    "        prompt_text = (\n",
    "            \"Analyze the orientation of this image. If the image appears to be rotated or incorrectly oriented \"\n",
    "            \"for typical viewing (e.g., text is sideways, objects are falling), indicate 'should_rotate' as true \"\n",
    "            \"and suggest the **clockwise** 'rotation_angle' in degrees (from 0, 90, 180, or 270) needed to make it upright. \"\n",
    "            \"If no rotation is needed, set 'should_rotate' to false and 'rotation_angle' to 0. \"\n",
    "            \"Provide a brief 'reason' for your assessment.\\n\"\n",
    "            \"Respond only with a JSON object like: \"\n",
    "            \"{'should_rotate': true/false, 'rotation_angle': 0/90/180/270, 'reason': '...'}\"\n",
    "        )\n",
    "\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[{\"text\": prompt_text}, image_part])\n",
    "        ]\n",
    "\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=0, # Keep low for deterministic orientation analysis\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"should_rotate\": {\"type\": \"BOOLEAN\"},\n",
    "                    \"rotation_angle\": {\"type\": \"STRING\", \"enum\": [\"0\", \"90\", \"180\", \"270\"]}, # Values are strings\n",
    "                    \"reason\": {\"type\": \"STRING\"}\n",
    "                },\n",
    "                \"required\": [\"should_rotate\", \"rotation_angle\", \"reason\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                parsed_json = json.loads(full_response_text)\n",
    "                if \"should_rotate\" in parsed_json and \"rotation_angle\" in parsed_json and \"reason\" in parsed_json:\n",
    "                    return parsed_json\n",
    "                else:\n",
    "                    return {\n",
    "                        \"error\": \"Unexpected JSON structure from API for rotation. Missing 'should_rotate', 'rotation_angle', or 'reason'.\",\n",
    "                        \"api_response\": parsed_json\n",
    "                    }\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": f\"Failed to parse JSON response from API for rotation: {e}\",\n",
    "                    \"raw_response_text\": full_response_text\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"No response text received from Gemini API for rotation analysis.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during rotation analysis API call: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": f\"An error occurred during rotation analysis API call: {e}\"}\n",
    "\n",
    "\n",
    "def get_bounding_boxes_gradio(image_pil: Image.Image, project_id: str, location: str) -> list[BoundingBox]:\n",
    "    \"\"\"\n",
    "    Detects bounding boxes of text regions or main objects in the image using Gemini API.\n",
    "    Returns a list of BoundingBox objects.\n",
    "    \"\"\"\n",
    "    if image_pil is None:\n",
    "        return []\n",
    "\n",
    "    img_bytes, mime_type = _infer_mime_type_and_convert_to_bytes(image_pil)\n",
    "    if img_bytes is None:\n",
    "        print(\"ERROR: Failed to convert image for bounding box detection API call.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"DEBUG: Bounding Box Detection - Image bytes length: {len(img_bytes)}, MIME type: {mime_type}\")\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "        try:\n",
    "            image_part = types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n",
    "        except Exception as e_part:\n",
    "            print(f\"ERROR: types.Part.from_bytes failed for bounding box detection: {e_part}\")\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "\n",
    "        prompt_text = (\n",
    "            \"Identify and return bounding boxes for significant text regions or main objects in this image. \"\n",
    "            \"Provide the bounding boxes as a JSON array of objects, where each object has 'box_2d' (normalized coordinates [y_min, x_min, y_max, x_max] from 0 to 1000) \"\n",
    "            \"and a descriptive 'label' for the object within the box.\"\n",
    "            \"Limit to 25 objects. If an object is present multiple times, give each object a unique label \"\n",
    "            \"according to its distinct characteristics (colors, size, position, etc..).\"\n",
    "        )\n",
    "\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[{\"text\": prompt_text}, image_part])\n",
    "        ]\n",
    "\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=0.5, # Allow some creativity for labels\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=list[BoundingBox] # Expect a list of BoundingBox objects\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                # pydantic's model_validate_json handles parsing and validation\n",
    "                parsed_boxes = [BoundingBox.model_validate(item) for item in json.loads(full_response_text)]\n",
    "                return parsed_boxes\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR: Failed to parse JSON response from API for bounding boxes: {e}\")\n",
    "                print(f\"Raw response text: {full_response_text}\")\n",
    "                return []\n",
    "            except Exception as e: # Catch Pydantic validation errors\n",
    "                print(f\"ERROR: Bounding box schema validation failed: {e}\")\n",
    "                traceback.print_exc()\n",
    "                print(f\"Raw response text: {full_response_text}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(\"WARNING: No response text received from Gemini API for bounding box detection.\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during bounding box detection API call: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "\n",
    "def analyze_image_enhancement_gradio(image_pil: Image.Image, project_id: str, location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes the image and suggests enhancements (brighten, sharpen, contrast change, upscale, noise removal).\n",
    "    Returns a dictionary with a list of suggested enhancements or an error.\n",
    "    \"\"\"\n",
    "    if image_pil is None:\n",
    "        return {\"error\": \"No image provided for enhancement analysis.\"}\n",
    "\n",
    "    img_bytes, mime_type = _infer_mime_type_and_convert_to_bytes(image_pil)\n",
    "    if img_bytes is None:\n",
    "        return {\"error\": \"Failed to convert image for API call (enhancement analysis).\"}\n",
    "\n",
    "    print(f\"DEBUG: Enhancement - Image bytes length: {len(img_bytes)}, MIME type: {mime_type}\")\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "        try:\n",
    "            image_part = types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n",
    "        except Exception as e_part:\n",
    "            print(f\"ERROR: types.Part.from_bytes failed for enhancement analysis: {e_part}\")\n",
    "            traceback.print_exc()\n",
    "            return {\"error\": f\"Failed to create image part for enhancement API: {e_part}\"}\n",
    "\n",
    "        prompt_text = (\n",
    "            \"Analyze this image and suggest necessary enhancements. \"\n",
    "            \"Provide a list of one or more suggestions from: \"\n",
    "            \"'brighten', 'sharpen', 'contrast change', 'upscale', 'noise removal'. \"\n",
    "            \"If no enhancement is needed, return an empty list. \"\n",
    "            \"Respond only with a JSON object containing a 'suggestions' key, which holds the list of strings.\"\n",
    "            \"Example: {'suggestions': ['brighten', 'noise removal']}\"\n",
    "        )\n",
    "\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[{\"text\": prompt_text}, image_part])\n",
    "        ]\n",
    "\n",
    "        # Define the response schema for a list of enhancement suggestions\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=0.5, # Allow some flexibility in suggestions\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"suggestions\": {\n",
    "                        \"type\": \"ARRAY\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"STRING\",\n",
    "                            \"enum\": [\"brighten\", \"sharpen\", \"contrast change\", \"upscale\", \"noise removal\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"suggestions\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                parsed_json = json.loads(full_response_text)\n",
    "                if \"suggestions\" in parsed_json and isinstance(parsed_json[\"suggestions\"], list):\n",
    "                    return parsed_json\n",
    "                else:\n",
    "                    return {\n",
    "                        \"error\": \"Unexpected JSON structure from API for enhancement. Missing 'suggestions' key or not a list.\",\n",
    "                        \"api_response\": parsed_json\n",
    "                    }\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": f\"Failed to parse JSON response from API for enhancement: {e}\",\n",
    "                    \"raw_response_text\": full_response_text\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"No response text received from Gemini API for enhancement analysis.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during enhancement analysis API call: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": f\"An error occurred during enhancement analysis API call: {e}\"}\n",
    "\n",
    "\n",
    "def extract_text_from_image_gradio(image_pil: Image.Image, project_id: str, location: str, keys_to_extract: list) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts specific text fields from the image (provided as PIL Image) using Gemini API\n",
    "    and returns them in a JSON format.\n",
    "    \"\"\"\n",
    "    if image_pil is None:\n",
    "        return {\"error\": \"No image provided for text extraction.\"}\n",
    "    if not keys_to_extract:\n",
    "        return {\"error\": \"No keys provided for extraction.\"}\n",
    "\n",
    "    img_bytes, mime_type = _infer_mime_type_and_convert_to_bytes(image_pil)\n",
    "    if img_bytes is None:\n",
    "        return {\"error\": \"Failed to convert image for API call.\"}\n",
    "\n",
    "    print(f\"DEBUG: Extraction - Image bytes length: {len(img_bytes)}, MIME type: {mime_type}\") # Debug print\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "        \n",
    "        # Changed from types.Part.from_data to types.Part.from_bytes as requested\n",
    "        try:\n",
    "            image_part = types.Part.from_bytes(data=img_bytes, mime_type=mime_type)\n",
    "        except Exception as e_part:\n",
    "            print(f\"ERROR: types.Part.from_bytes failed: {e_part}\")\n",
    "            traceback.print_exc() # Print full traceback here for detailed error\n",
    "            return {\"error\": f\"Failed to create image part for API: {e_part}\"}\n",
    "\n",
    "        prompt_text = (\n",
    "            f\"Extract the following information from this image and provide it as a JSON object: \"\n",
    "            f\"{', '.join(keys_to_extract)}. If a piece of information is not found, use 'N/A' as its value. \"\n",
    "            \"Ensure the keys in the JSON exactly match the requested information and their casing.\"\n",
    "            \"\\nExample: {'Name': 'John Doe', 'Father\\'s Name': 'N/A', 'PAN Number': 'ABCDE1234F', 'Date of birth': '1990-01-01'}\"\n",
    "        )\n",
    "\n",
    "        contents = [\n",
    "            types.Content(role=\"user\", parts=[{\"text\": prompt_text}, image_part])\n",
    "        ]\n",
    "\n",
    "        response_schema_properties = {}\n",
    "        for key in keys_to_extract:\n",
    "            schema_key = re.sub(r'[^a-zA-Z0-9_]', '', key.replace(\" \", \"_\")).lower()\n",
    "            response_schema_properties[schema_key] = {\"type\": \"STRING\"}\n",
    "        \n",
    "        if \"Date of birth\" in keys_to_extract:\n",
    "             response_schema_properties[\"date_of_birth\"] = {\"type\": \"STRING\", \"pattern\": \"^\\\\d{4}-\\\\d{2}-\\\\d{2}$|^N/A$\"}\n",
    "\n",
    "\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=0.2, # Keep low for factual extraction\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": response_schema_properties,\n",
    "                \"required\": [re.sub(r'[^a-zA-Z0-9_]', '', key.replace(\" \", \"_\")).lower() for key in keys_to_extract]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                parsed_json = json.loads(full_response_text)\n",
    "                \n",
    "                final_extracted_data = {}\n",
    "                for original_key in keys_to_extract:\n",
    "                    schema_key = re.sub(r'[^a-zA-Z0-9_]', '', original_key.replace(\" \", \"_\")).lower()\n",
    "                    final_extracted_data[original_key] = parsed_json.get(schema_key, \"N/A\")\n",
    "\n",
    "                return final_extracted_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": f\"Failed to parse JSON response from API: {e}\",\n",
    "                    \"raw_response_text\": full_response_text\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"No structured text extracted from the image by Gemini API.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during text extraction API call: {e}\")\n",
    "        traceback.print_exc() # Print full traceback here for detailed error\n",
    "        return {\"error\": f\"An error occurred during text extraction API call: {e}\"}\n",
    "\n",
    "    \n",
    "def verify_extracted_data(extracted_text_json: dict, keys_to_verify: list, project_id: str, location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Verifies if specific keys (like name, father's name, PAN, DOB) are present\n",
    "    and valid in the *structured* extracted data.\n",
    "    \"\"\"\n",
    "    if not extracted_text_json or not isinstance(extracted_text_json, dict):\n",
    "        return {\"error\": \"Invalid or no extracted JSON data provided for verification.\"}\n",
    "\n",
    "    if not keys_to_verify:\n",
    "        return {\"error\": \"No keys provided for verification.\"}\n",
    "\n",
    "    try:\n",
    "        client = genai.Client(vertexai=True, project=project_id, location=location)\n",
    "        model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "        extracted_json_str = json.dumps(extracted_text_json, indent=2)\n",
    "\n",
    "        verification_prompt = (\n",
    "            f\"Given the following structured extracted data:\\n\\n-----{extracted_json_str}\\n---\\n\\n\"\n",
    "            \"For each of the following pieces of information, state if its value is meaningfully present \"\n",
    "            \"and not 'N/A' or empty. Provide your answer as a JSON object \"\n",
    "            \"where keys are the requested information and values are 'Yes' or 'No'.\\n\\n\"\n",
    "            f\"Information to verify: {', '.join(keys_to_verify)}\\n\\n\"\n",
    "            \"Example: {'Name': 'Yes', 'Address': 'No'}\"\n",
    "        )\n",
    "\n",
    "        contents = [types.Content(role=\"user\", parts=[{\"text\": verification_prompt}])]\n",
    "\n",
    "        response_schema_properties = {\n",
    "            re.sub(r'[^a-zA-Z0-9_]', '', key.replace(\" \", \"_\")).lower(): {\"type\": \"STRING\", \"enum\": [\"Yes\", \"No\"]} for key in keys_to_verify\n",
    "        }\n",
    "\n",
    "        generation_config = types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            seed=0,\n",
    "            max_output_tokens=65535,\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n",
    "                types.SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n",
    "                # types.SafetySetting(category=\"HARASSMENT\", threshold=\"OFF\") # Removed HARASSMENT\n",
    "            ],\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=-1),\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema={\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": response_schema_properties\n",
    "            }\n",
    "        )\n",
    "\n",
    "        full_response_text = \"\"\n",
    "        for chunk in client.models.generate_content_stream(\n",
    "            model=model_name,\n",
    "            contents=contents,\n",
    "            config=generation_config,\n",
    "        ):\n",
    "            if chunk.text:\n",
    "                full_response_text += chunk.text\n",
    "\n",
    "        if full_response_text:\n",
    "            try:\n",
    "                parsed_json = json.loads(full_response_text)\n",
    "                result = {}\n",
    "                for key in keys_to_verify:\n",
    "                    schema_key = re.sub(r'[^a-zA-Z0-9_]', '', key.replace(\" \", \"_\")).lower()\n",
    "                    result[key] = parsed_json.get(schema_key, \"No\")\n",
    "                return result\n",
    "            except json.JSONDecodeError as e:\n",
    "                return {\n",
    "                    \"error\": f\"Failed to parse JSON response from API during verification: {e}\",\n",
    "                    \"raw_response_text\": full_response_text\n",
    "                }\n",
    "        else:\n",
    "            return {\"error\": \"No response text received from Gemini API for verification.\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: An error occurred during verification API call: {e}\")\n",
    "        traceback.print_exc() # Print full traceback here for detailed error\n",
    "        return {\"error\": f\"An error occurred during verification API call: {e}\"}\n",
    "\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "# Changed location from 'global' to 'us-central1' for more explicit regional targeting,\n",
    "# as it's a common and well-supported region for Vertex AI.\n",
    "LOCATION = \"us-central1\" \n",
    "KEYS_OF_INTEREST = [\"Name\", \"Father's Name\", \"PAN Number\", \"Date of birth\"]\n",
    "\n",
    "def process_image(image_pil: Image.Image, extract_data: bool, validate_data: bool, analyze_rotation: bool, analyze_enhancement: bool, enable_cropping: bool):\n",
    "    \"\"\"\n",
    "    Main function to process the uploaded image through the different stages.\n",
    "    Ensures all outputs for gr.JSON are JSON strings.\n",
    "    Applies rotation first, then crops (conditionally), then sends for extraction/validation.\n",
    "    \"\"\"\n",
    "    # Initialize all results as empty JSON strings\n",
    "    readability_output = json.dumps({})\n",
    "    rotation_analysis_output = json.dumps({})\n",
    "    enhancement_suggestions_output = json.dumps({})\n",
    "    \n",
    "    # Initialize image outputs\n",
    "    rotated_image_display = None\n",
    "    cropped_image_display = None\n",
    "\n",
    "    if image_pil is None:\n",
    "        readability_output = json.dumps({\"error\": \"Please upload an image.\"})\n",
    "        # Return gr.update for textboxes, setting initial empty value and no class\n",
    "        return readability_output, rotation_analysis_output, rotated_image_display, cropped_image_display, enhancement_suggestions_output, \\\n",
    "               gr.update(value=\"\", elem_classes=[\"unvalidated-field\"]), \\\n",
    "               gr.update(value=\"\", elem_classes=[\"unvalidated-field\"]), \\\n",
    "               gr.update(value=\"\", elem_classes=[\"unvalidated-field\"]), \\\n",
    "               gr.update(value=\"\", elem_classes=[\"unvalidated-field\"])\n",
    "\n",
    "    original_image_pil = image_pil\n",
    "    image_after_rotation = original_image_pil # This will be modified if rotation occurs\n",
    "\n",
    "    # Stage 1: Readability Check (always on original image)\n",
    "    readability_response_dict = analyze_image_text_readability_gradio(original_image_pil, PROJECT_ID, LOCATION)\n",
    "    readability_output = json.dumps(readability_response_dict, indent=2)\n",
    "\n",
    "    # Stage 2: Perform Rotation Analysis on the ORIGINAL image FIRST\n",
    "    rotation_angle_to_apply = 0 # Default no rotation\n",
    "    if analyze_rotation:\n",
    "        rotation_analysis_response_dict = analyze_image_rotation_gradio(original_image_pil, PROJECT_ID, LOCATION) # Analyze original image\n",
    "        rotation_analysis_output = json.dumps(rotation_analysis_response_dict, indent=2)\n",
    "\n",
    "        should_rotate = rotation_analysis_response_dict.get(\"should_rotate\", False)\n",
    "        predicted_angle_str = rotation_analysis_response_dict.get(\"rotation_angle\")\n",
    "\n",
    "        if should_rotate and predicted_angle_str in [\"0\", \"90\", \"180\", \"270\"]:\n",
    "            try:\n",
    "                # Map desired CLOCKWISE angle to PIL's expected COUNTER-CLOCKWISE angle\n",
    "                # PIL.Image.rotate(angle) rotates counter-clockwise\n",
    "                # To achieve X degrees clockwise, we need to rotate by -X degrees counter-clockwise\n",
    "                pil_rotation_map = {\n",
    "                    \"0\": 0,\n",
    "                    \"90\": -90,  # 90 degrees clockwise is -90 degrees counter-clockwise\n",
    "                    \"180\": -180, # 180 degrees clockwise is -180 degrees counter-clockwise\n",
    "                    \"270\": -270 # 270 degrees clockwise is -270 degrees counter-clockwise\n",
    "                }\n",
    "                rotation_angle_for_pil = pil_rotation_map.get(predicted_angle_str, 0) # Default to 0 if invalid\n",
    "\n",
    "                if rotation_angle_for_pil != 0: # Only rotate if there's an actual rotation needed\n",
    "                    image_after_rotation = original_image_pil.rotate(rotation_angle_for_pil, expand=True)\n",
    "                    rotated_image_display = image_after_rotation # Display this rotated image\n",
    "                    print(f\"DEBUG: Original image rotated by {predicted_angle_str} degrees CLOCKWISE.\")\n",
    "                else:\n",
    "                    print(\"DEBUG: Rotation analysis recommended 0-degree rotation, no actual rotation applied.\")\n",
    "                    rotated_image_display = original_image_pil # Display original if 0 rotation\n",
    "            except ValueError:\n",
    "                print(f\"WARNING: Invalid rotation_angle received from API (not an integer string): {predicted_angle_str}\")\n",
    "                rotated_image_display = original_image_pil # Display original on error\n",
    "        else:\n",
    "            print(\"DEBUG: Rotation analysis did not recommend rotation or angle was invalid/not found.\")\n",
    "            rotated_image_display = original_image_pil # Display original if no rotation/invalid angle\n",
    "    else:\n",
    "        rotation_analysis_output = json.dumps({\"status\": \"Rotation analysis skipped\"})\n",
    "        rotated_image_display = original_image_pil # Display original if rotation analysis is skipped\n",
    "\n",
    "    # Now, determine the image to be used for subsequent stages\n",
    "    image_for_extraction_validation = image_after_rotation # Default to rotated image\n",
    "\n",
    "    # Stage 3: Get Bounding Boxes and Optionally Crop (on the image_after_rotation)\n",
    "    if enable_cropping: # Only perform bounding box detection and cropping if enabled\n",
    "        bounding_boxes = []\n",
    "        # Only detect boxes if extraction/validation/enhancement is intended OR if rotation analysis was active (to potentially crop before extraction)\n",
    "        if (extract_data or validate_data or analyze_enhancement) and analyze_rotation: # Added analyze_enhancement condition\n",
    "            bounding_boxes = get_bounding_boxes_gradio(image_after_rotation, PROJECT_ID, LOCATION) # Detect boxes on ROTATED image\n",
    "\n",
    "            if bounding_boxes:\n",
    "                largest_bbox = None\n",
    "                max_area = -1\n",
    "                width_px, height_px = image_after_rotation.size\n",
    "\n",
    "                for bbox in bounding_boxes:\n",
    "                    y_min, x_min, y_max, x_max = bbox.box_2d\n",
    "                    abs_x_min = int(x_min / 1000 * width_px)\n",
    "                    abs_y_min = int(y_min / 1000 * height_px)\n",
    "                    abs_x_max = int(x_max / 1000 * width_px)\n",
    "                    abs_y_max = int(y_max / 1000 * height_px)\n",
    "\n",
    "                    current_area = (abs_x_max - abs_x_min) * (abs_y_max - abs_y_min)\n",
    "                    if current_area > max_area:\n",
    "                        max_area = current_area\n",
    "                        largest_bbox = (abs_x_min, abs_y_min, abs_x_max, abs_y_max)\n",
    "                \n",
    "                if largest_bbox:\n",
    "                    # Crop the rotated image using the largest bounding box\n",
    "                    image_for_extraction_validation = image_after_rotation.crop(largest_bbox)\n",
    "                    cropped_image_display = image_for_extraction_validation # Display this cropped image\n",
    "                    print(f\"DEBUG: Rotated image cropped to largest bounding box for extraction/validation: {largest_bbox}\")\n",
    "                else:\n",
    "                    print(\"DEBUG: No valid bounding boxes found on rotated image for cropping. Using full rotated image for extraction.\")\n",
    "                    cropped_image_display = image_after_rotation # Display full rotated image if no crop but cropping enabled\n",
    "            else:\n",
    "                print(\"DEBUG: No bounding boxes detected on rotated image. Using full rotated image for extraction.\")\n",
    "                cropped_image_display = image_after_rotation # Display full rotated image if no crop but cropping enabled\n",
    "        else:\n",
    "            # If cropping is enabled but conditions for bounding box detection are not met\n",
    "            cropped_image_display = image_after_rotation # Display the rotated image, no cropping happened\n",
    "    else: # Cropping is disabled\n",
    "        print(\"DEBUG: Cropping disabled by user. Using full rotated image for extraction/validation/enhancement.\")\n",
    "        image_for_extraction_validation = image_after_rotation # Use the full rotated image\n",
    "        cropped_image_display = image_after_rotation # Display the full rotated image\n",
    "\n",
    "\n",
    "    # Stage 4: Conditional Image Enhancement Analysis (on the final processed image)\n",
    "    if analyze_enhancement:\n",
    "        enhancement_response_dict = analyze_image_enhancement_gradio(image_for_extraction_validation, PROJECT_ID, LOCATION)\n",
    "        enhancement_suggestions_output = json.dumps(enhancement_response_dict, indent=2)\n",
    "    else:\n",
    "        enhancement_suggestions_output = json.dumps({\"status\": \"Enhancement analysis skipped\"})\n",
    "\n",
    "\n",
    "    # Stage 5: Conditional Text Extraction (now uses potentially rotated AND cropped image)\n",
    "    extracted_data_response_dict = {}\n",
    "    is_readable = readability_response_dict.get(\"is_readable\") # Still refer to original readability\n",
    "    if is_readable and extract_data:\n",
    "        extracted_data_response_dict = extract_text_from_image_gradio(image_for_extraction_validation, PROJECT_ID, LOCATION, KEYS_OF_INTEREST)\n",
    "    elif extract_data:\n",
    "        # If extraction requested but original image not readable, populate default values for display\n",
    "        extracted_data_response_dict = {key: \"N/A\" for key in KEYS_OF_INTEREST}\n",
    "\n",
    "\n",
    "    # Stage 6: Conditional Data Verification\n",
    "    verified_data_response_dict = {}\n",
    "    if extract_data and validate_data: # Only validate if extraction was attempted\n",
    "        extraction_successful = extracted_data_response_dict and not extracted_data_response_dict.get(\"error\")\n",
    "        if extraction_successful:\n",
    "            verified_data_response_dict = verify_extracted_data(extracted_data_response_dict, KEYS_OF_INTEREST, PROJECT_ID, LOCATION)\n",
    "        else:\n",
    "            # If extraction failed, set all verification to \"No\" for display clarity\n",
    "            verified_data_response_dict = {key: \"No\" for key in KEYS_OF_INTEREST}\n",
    "    elif validate_data: # If validation requested but extraction not, default all to \"No\"\n",
    "        verified_data_response_dict = {key: \"No\" for key in KEYS_OF_INTEREST}\n",
    "\n",
    "\n",
    "    # Populate individual text fields and their validation classes\n",
    "    name_value = extracted_data_response_dict.get(\"Name\", \"N/A\")\n",
    "    father_name_value = extracted_data_response_dict.get(\"Father's Name\", \"N/A\")\n",
    "    pan_number_value = extracted_data_response_dict.get(\"PAN Number\", \"N/A\")\n",
    "    dob_value = extracted_data_response_dict.get(\"Date of birth\", \"N/A\")\n",
    "\n",
    "    # Determine CSS classes AND append tick/cross marks based on validation result\n",
    "    name_validation_class = \"valid-field\" if verified_data_response_dict.get(\"Name\") == \"Yes\" else (\"invalid-field\" if validate_data else \"\")\n",
    "    father_name_validation_class = \"valid-field\" if verified_data_response_dict.get(\"Father's Name\") == \"Yes\" else (\"invalid-field\" if validate_data else \"\")\n",
    "    pan_number_validation_class = \"valid-field\" if verified_data_response_dict.get(\"PAN Number\") == \"Yes\" else (\"invalid-field\" if validate_data else \"\")\n",
    "    dob_validation_class = \"valid-field\" if verified_data_response_dict.get(\"Date of birth\") == \"Yes\" else (\"invalid-field\" if validate_data else \"\")\n",
    "\n",
    "    if validate_data:\n",
    "        name_value += \" ✔\" if verified_data_response_dict.get(\"Name\") == \"Yes\" else \" ✘\"\n",
    "        father_name_value += \" ✔\" if verified_data_response_dict.get(\"Father's Name\") == \"Yes\" else \" ✘\"\n",
    "        pan_number_value += \" ✔\" if verified_data_response_dict.get(\"PAN Number\") == \"Yes\" else \" ✘\"\n",
    "        dob_value += \" ✔\" if verified_data_response_dict.get(\"Date of birth\") == \"Yes\" else \" ✘\"\n",
    "\n",
    "\n",
    "    # Return all outputs as gr.update objects for the textboxes to set value and class\n",
    "    return readability_output, rotation_analysis_output, rotated_image_display, cropped_image_display, enhancement_suggestions_output, \\\n",
    "           gr.update(value=name_value, elem_classes=[name_validation_class]), \\\n",
    "           gr.update(value=father_name_value, elem_classes=[father_name_validation_class]), \\\n",
    "           gr.update(value=pan_number_value, elem_classes=[pan_number_validation_class]), \\\n",
    "           gr.update(value=dob_value, elem_classes=[dob_validation_class])\n",
    "\n",
    "# Gradio Interface setup\n",
    "# Custom CSS for validation styling\n",
    "custom_css = \"\"\"\n",
    ".valid-field {\n",
    "    border: 2px solid green !important;\n",
    "    background-color: #e6ffe6 !important; /* Light green background */\n",
    "}\n",
    ".invalid-field {\n",
    "    border: 2px solid red !important;\n",
    "    background-color: #ffe6e6 !important; /* Light red background */\n",
    "}\n",
    "/* Optional: style for textboxes that haven't been validated yet */\n",
    ".unvalidated-field {\n",
    "    border: 1px solid #ccc !important;\n",
    "    background-color: #f0f0f0 !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=custom_css) as demo:\n",
    "    gr.Markdown(\"# Intelligent Document Processor\") # Changed title here\n",
    "    gr.Markdown(\"Upload an image to check its readability, analyze its orientation for rotation, get enhancement suggestions, and extract/validate data.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Original Image\")\n",
    "            analyze_rotation_checkbox = gr.Checkbox(label=\"Analyze & Auto-Rotate Image\", value=True)\n",
    "            enable_cropping_checkbox = gr.Checkbox(label=\"Enable Cropping (after rotation)\", value=True) # New checkbox\n",
    "            analyze_enhancement_checkbox = gr.Checkbox(label=\"Analyze Image Enhancement Needs\", value=True)\n",
    "            extract_checkbox = gr.Checkbox(label=\"Extract Data\", value=True)\n",
    "            validate_checkbox = gr.Checkbox(label=\"Validate Extracted Data\", value=True)\n",
    "            process_button = gr.Button(\"Analyze Image\")\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"---\") # Separator\n",
    "            gr.Markdown(\"## Extracted & Verified Data\")\n",
    "            with gr.Row():\n",
    "                # Textboxes for Name and Father's Name\n",
    "                name_box = gr.Textbox(label=\"Name\", interactive=False, elem_id=\"name_box\")\n",
    "                father_name_box = gr.Textbox(label=\"Father's Name\", interactive=False, elem_id=\"father_name_box\")\n",
    "            with gr.Row():\n",
    "                # Textboxes for PAN Number and Date of Birth\n",
    "                pan_box = gr.Textbox(label=\"PAN Number\", interactive=False, elem_id=\"pan_box\")\n",
    "                dob_box = gr.Textbox(label=\"Date of Birth\", interactive=False, elem_id=\"dob_box\")\n",
    "\n",
    "            gr.Markdown(\"---\") # Separator\n",
    "            readability_output = gr.JSON(label=\"Readability Analysis (Original Image)\")\n",
    "            rotation_analysis_output = gr.JSON(label=\"Image Rotation Analysis (on Original Image)\")\n",
    "            enhancement_suggestions_output = gr.JSON(label=\"Image Enhancement Suggestions (on Final Processed Image)\")\n",
    "            rotated_image_display = gr.Image(type=\"pil\", label=\"Image After Rotation (if applied)\", show_label=True, visible=True)\n",
    "            cropped_image_display = gr.Image(type=\"pil\", label=\"Cropped Image for Analysis (after rotation)\", show_label=True, visible=True)\n",
    "            \n",
    "    process_button.click(\n",
    "        process_image,\n",
    "        inputs=[image_input, extract_checkbox, validate_checkbox, analyze_rotation_checkbox, analyze_enhancement_checkbox, enable_cropping_checkbox],\n",
    "        outputs=[\n",
    "            readability_output,\n",
    "            rotation_analysis_output,\n",
    "            rotated_image_display,\n",
    "            cropped_image_display,\n",
    "            enhancement_suggestions_output,\n",
    "            name_box,\n",
    "            father_name_box,\n",
    "            pan_box,\n",
    "            dob_box,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True, share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf21c3-ea88-4bc0-b0da-e13f18d864c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
